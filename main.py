__description__ = "Dub Analysis & Tagging."
__author__ = "BASSHOUS3"
__version__ = "0.4.21" #current_mtime update by lavacano

import re
import os
import sys
import time
import json
import argparse
import pycountry
import requests
from datetime import datetime
from pymediainfo import MediaInfo
import xml.etree.ElementTree as ET
import logging
from dotenv import load_dotenv

load_dotenv()

# === CONFIG ===
SONARR_API_KEY = os.getenv("SONARR_API_KEY")
SONARR_URL = os.getenv("SONARR_URL")
ROOT_TV_PATH = os.getenv("ROOT_TV_PATH")
TAGGARR_JSON_PATH = os.path.join(ROOT_TV_PATH, "taggarr.json")
RUN_INTERVAL_SECONDS = int(os.getenv("RUN_INTERVAL_SECONDS", 7200))
START_RUNNING = os.getenv("START_RUNNING", "true").lower() == "true"
QUICK_MODE = os.getenv("QUICK_MODE", "false").lower() == "true"
DRY_RUN = os.getenv("DRY_RUN", "false").lower() == "true"
WRITE_MODE = int(os.getenv("WRITE_MODE", 0))
TAG_DUB = os.getenv("TAG_DUB", "dub")
TAG_SEMI = os.getenv("TAG_SEMI", "semi-dub")
TAG_WRONG_DUB = os.getenv("TAG_WRONG", "wrong-dub")
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
LOG_PATH = os.getenv("LOG_PATH", "/logs")
TARGET_GENRE = os.getenv("TARGET_GENRE")
TARGET_LANGUAGES = [lang.strip().lower() for lang in os.getenv("TARGET_LANGUAGES", "en").split(",")]
ADD_TAG_TO_GENRE = os.getenv("ADD_TAG_TO_GENRE", "false").lower() == "true"

# Radarr config
RADARR_API_KEY = os.getenv("RADARR_API_KEY")
RADARR_URL = os.getenv("RADARR_URL")
ROOT_MOVIE_PATH = os.getenv("ROOT_MOVIE_PATH")
TARGET_GENRE_MOVIES = os.getenv("TARGET_GENRE_MOVIES")
TAGGARR_MOVIES_JSON_PATH = os.path.join(ROOT_MOVIE_PATH, "taggarr.json") if ROOT_MOVIE_PATH else None

# Service detection
SONARR_ENABLED = all([SONARR_API_KEY, SONARR_URL, ROOT_TV_PATH])
RADARR_ENABLED = all([RADARR_API_KEY, RADARR_URL, ROOT_MOVIE_PATH])


# === LOGGING ===
def setup_logging():
    log_dir =  LOG_PATH
    os.makedirs(log_dir, exist_ok=True)
    log_file = os.path.join(LOG_PATH, f"taggarr({__version__})_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

    logger = logging.getLogger()
    logger.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))

    file_handler = logging.FileHandler(log_file)
    stream_handler = logging.StreamHandler()

    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    stream_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)
    #logger.info(f"üè∑Ô∏è Taggarr - {__description__}")
    time.sleep(1)
    #logger.info(f"üè∑Ô∏è Taggarr - v{__version__} started.")
    time.sleep(3)
    logger.debug(f"Log file created: {log_file}")
    size_bytes = os.path.getsize(log_file)
    size_mb = size_bytes / (1024 * 1024)
    #logger.debug(f"Log file size: {size_mb:.2f} MB")

    return logger

logger = setup_logging()

# === JSON STORAGE ===
def load_taggarr():
    if os.path.exists(TAGGARR_JSON_PATH):
        try:
            logger.info(f"üìç taggarr.json found at {TAGGARR_JSON_PATH}")
            with open(TAGGARR_JSON_PATH, 'r') as f:
                data = json.load(f)
                logger.debug(f"‚úÖ Loaded taggarr.json with {len(data.get('series', {}))} entries.")
                return data
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è taggarr.json is corrupted: {e}")
            backup_path = TAGGARR_JSON_PATH + ".bak"
            os.rename(TAGGARR_JSON_PATH, backup_path)
            logger.warning(f"‚ùå Corrupted file moved to: {backup_path}")

    logger.info("‚ùå No taggarr.json found ‚Äî starting fresh.")
    return {"series": {}}


def save_taggarr(data):
    try:
        data["version"] = __version__
        ordered_data = {"version": data["version"]}
        for k, v in data.items():
            if k != "version":
                ordered_data[k] = v
        raw_json = json.dumps(ordered_data, indent=2, ensure_ascii=False)

        # compact E## lists
        compact_json = re.sub(
            r'(\[\s*\n\s*)((?:\s*"E\d{2}",?\s*\n?)+)(\s*\])',
            lambda m: '[{}]'.format(
                ', '.join(re.findall(r'"E\d{2}"', m.group(2)))
            ),
            raw_json
        )

        # compact dub/missing_dub/original_dub lists
        compact_json = re.sub(
            r'("original_dub": |\s*"dub": |\s*"missing_dub": |\s*"unexpected_languages": )\[\s*\n\s*((?:\s*"[^"]+",?\s*\n?)+)(\s*\])',
            lambda m: '{}[{}]'.format(
                m.group(1),
                ', '.join(f'"{x}"' for x in re.findall(r'"([^"]+)"', m.group(2)))
            ),
            compact_json
        )

        with open(TAGGARR_JSON_PATH, 'w') as f:
            f.write(compact_json)
        logger.debug("‚úÖ taggarr.json saved successfully.")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Failed to save taggarr.json: {e}")

# === MOVIE JSON STORAGE ===
def load_taggarr_movies():
    if not TAGGARR_MOVIES_JSON_PATH:
        return {"movies": {}}
    if os.path.exists(TAGGARR_MOVIES_JSON_PATH):
        try:
            logger.info(f"üìç taggarr.json (movies) found at {TAGGARR_MOVIES_JSON_PATH}")
            with open(TAGGARR_MOVIES_JSON_PATH, 'r') as f:
                data = json.load(f)
                logger.debug(f"‚úÖ Loaded taggarr.json (movies) with {len(data.get('movies', {}))} entries.")
                return data
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è taggarr.json (movies) is corrupted: {e}")
            backup_path = TAGGARR_MOVIES_JSON_PATH + ".bak"
            os.rename(TAGGARR_MOVIES_JSON_PATH, backup_path)
            logger.warning(f"‚ùå Corrupted file moved to: {backup_path}")

    logger.info("‚ùå No taggarr.json (movies) found ‚Äî starting fresh.")
    return {"movies": {}}


def save_taggarr_movies(data):
    if not TAGGARR_MOVIES_JSON_PATH:
        return
    try:
        data["version"] = __version__
        ordered_data = {"version": data["version"]}
        for k, v in data.items():
            if k != "version":
                ordered_data[k] = v
        raw_json = json.dumps(ordered_data, indent=2, ensure_ascii=False)

        # compact language lists
        compact_json = re.sub(
            r'("languages": )\[\s*\n\s*((?:\s*"[^"]+",?\s*\n?)+)(\s*\])',
            lambda m: '{}[{}]'.format(
                m.group(1),
                ', '.join(f'"{x}"' for x in re.findall(r'"([^"]+)"', m.group(2)))
            ),
            raw_json
        )

        with open(TAGGARR_MOVIES_JSON_PATH, 'w') as f:
            f.write(compact_json)
        logger.debug("‚úÖ taggarr.json (movies) saved successfully.")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Failed to save taggarr.json (movies): {e}")

# === MEDIA TOOLS ===
def analyze_audio(video_path):
    try:
        media_info = MediaInfo.parse(video_path)
        langs = set()
        fallback_detected = False

        for t in media_info.tracks:
            if t.track_type == "Audio":
                lang = (t.language or "").strip().lower()
                title = (t.title or "").strip().lower()

                if lang:
                    langs.add(lang)
                elif "track 1" in title or "audio 1" in title or title == "":
                    langs.add("__fallback_original__")
                    fallback_detected = True

        logger.debug(f"Analyzed {video_path}, found audio languages: {sorted(langs)}")
        if fallback_detected:
            logger.debug(f"Fallback language detection used in {video_path}")
        return list(langs)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Audio analysis failed for {video_path}: {e}")
        return []


def scan_movie(movie_path, movie_meta):
    """
    Scan a movie folder and return language analysis.
    Finds the largest video file (main feature) and analyzes its audio tracks.
    """
    video_exts = ['.mkv', '.mp4', '.m4v', '.avi', '.webm', '.mov', '.mxf']
    ignore_patterns = ['-sample', 'sample.', 'extras', 'featurettes', 'behind the scenes', 'deleted scenes']

    # Find all video files in movie folder
    video_files = []
    for root, dirs, files in os.walk(movie_path):
        # Skip ignored directories
        dirs[:] = [d for d in dirs if not any(p in d.lower() for p in ignore_patterns)]
        for f in files:
            if os.path.splitext(f)[1].lower() in video_exts:
                # Skip sample files
                if any(p in f.lower() for p in ignore_patterns):
                    continue
                full_path = os.path.join(root, f)
                video_files.append((full_path, os.path.getsize(full_path)))

    if not video_files:
        logger.warning(f"No video files found in {movie_path}")
        return None

    # Get the largest file (main feature)
    main_file = max(video_files, key=lambda x: x[1])[0]
    logger.debug(f"Scanning main movie file: {os.path.basename(main_file)}")

    langs = analyze_audio(main_file)

    # Get original language from Radarr metadata
    original_lang = movie_meta.get("originalLanguage", {})
    if isinstance(original_lang, dict):
        original_lang_name = original_lang.get("name", "").lower()
    else:
        original_lang_name = str(original_lang).lower()

    ORIGINAL_LANGUAGE_CODES = get_language_aliases(original_lang_name)

    return {
        "file": os.path.basename(main_file),
        "languages": langs,
        "original_language": original_lang_name,
        "original_codes": ORIGINAL_LANGUAGE_CODES,
        "last_modified": os.path.getmtime(main_file)
    }


def scan_season(season_path, show, quick=False):
    video_exts = ['.mkv', '.mp4', '.m4v', '.avi', '.webm', '.mov', '.mxf']
    files = sorted([f for f in os.listdir(season_path) if os.path.splitext(f)[1].lower() in video_exts])
    if quick and files:
        files = [files[0]]

    original_lang = show.get("originalLanguage", "")
    if isinstance(original_lang, dict):
        original_lang_name = original_lang.get("name", "").lower()
    else:
        original_lang_name = str(original_lang).lower()

    ORIGINAL_LANGUAGE_CODES = get_language_aliases(original_lang_name)
    ACCEPTED_LANGUAGES = LANGUAGE_CODES.union(ORIGINAL_LANGUAGE_CODES)

    stats = {
        "episodes": len(files) if not quick else 1,
        "original_dub": [],
        "dub": [],
        "missing_dub": [],
        "unexpected_languages": [],
    }

    for f in files:
        full_path = os.path.join(season_path, f)
        langs = analyze_audio(full_path)

        match = re.search(r'(E\d{2})', f, re.IGNORECASE)
        ep_name = match.group(1) if match else os.path.splitext(f)[0]

        # Handle fallback audio track assumption
        if "__fallback_original__" in langs:
            stats["original_dub"].append(ep_name)
            logger.info(f"‚ö†Ô∏èüîä Audio track not labelled for {ep_name} ‚Äî using fallback: assuming audio is original language.")
            continue  # skip further analysis for this episode

        langs_set = set(langs)
        has_target = langs_set.intersection(LANGUAGE_CODES)

        # Build language aliases for the current file's audio tracks
        langs_aliases = set()
        for l in langs:
            langs_aliases.update(get_language_aliases(l))

        missing_target = set()
        for t in TARGET_LANGUAGES:
            t_aliases = get_language_aliases(t)
            if not langs_aliases.intersection(t_aliases):
                missing_target.add(t)

        has_original = langs_set.intersection(ORIGINAL_LANGUAGE_CODES)

        if has_original:
            stats["original_dub"].append(ep_name)
        if has_target:
            stats["dub"].append(f"{ep_name}:{', '.join(sorted(has_target))}")
        if missing_target:
            short_missing = [get_primary_iso_code(m) for m in sorted(missing_target)]
            stats["missing_dub"].append(f"{ep_name}:{', '.join(short_missing)}")

        # collect unexpected langs
        for l in langs:
            if l not in LANGUAGE_CODES and l not in ORIGINAL_LANGUAGE_CODES:
                stats["unexpected_languages"].append(l)

    stats["unexpected_languages"] = sorted(set(stats["unexpected_languages"]))
    return stats




def determine_tag_and_stats(show_path, show, quick=False): #tag method handling is very delicate
    seasons = {}
    has_wrong_dub = False
    has_dub = False
    season_stats = {}

    for entry in os.listdir(show_path):
        season_path = os.path.join(show_path, entry)
        if os.path.isdir(season_path) and entry.lower().startswith("season"):
            logger.info(f"Scanning season: {entry}")
            stats = scan_season(season_path, show, quick=quick)
            stats["last_modified"] = os.path.getmtime(season_path)

            has_any_dub = bool(stats["dub"])
            has_any_wrong = bool(stats["unexpected_languages"])
            has_dub = has_dub or has_any_dub
            has_wrong_dub = has_wrong_dub or has_any_wrong

            if has_any_wrong:
                stats["status"] = "wrong-dub"
            elif not stats["missing_dub"] and stats["dub"]:
                stats["status"] = "fully-dub"
            else:
                stats["status"] = "semi-dub" if stats["dub"] else "original"

            season_stats[entry] = stats

    for season in sorted(season_stats.keys()):
        seasons[season] = season_stats[season]
    final_statuses = [s["status"] for s in seasons.values()]

    if has_wrong_dub:
        return TAG_WRONG_DUB, seasons
    elif all(s == "fully-dub" for s in final_statuses):
        return TAG_DUB, seasons
    elif any(s in ("fully-dub", "semi-dub") for s in final_statuses):
        return TAG_SEMI, seasons

    return None, seasons



# === LANGUAGE HANDLING ===

def get_language_aliases(code_or_name):
    aliases = set()
    if not code_or_name:
        return aliases
    code_or_name = code_or_name.lower()

    try:
        lang = (
            pycountry.languages.get(alpha_2=code_or_name)
            or pycountry.languages.get(alpha_3=code_or_name)
            or pycountry.languages.lookup(code_or_name)
        )
    except Exception:
        lang = None

    if lang:
        if hasattr(lang, 'alpha_2'):
            aliases.add(lang.alpha_2.lower())
        if hasattr(lang, 'alpha_3'):
            aliases.add(lang.alpha_3.lower())
        aliases.add(lang.name.lower())

    for suffix in ['-us', '-gb', '-ca', '-au', '-fr', '-de', '-jp', '-kr', '-cn', '-tw', '-ru']:
        aliases.update(a + suffix for a in list(aliases))

    return aliases

            # Flatten all aliases from target languages
LANGUAGE_CODES = set()
for lang in TARGET_LANGUAGES:
    LANGUAGE_CODES.update(get_language_aliases(lang))

            # shorten user entries to avoid long massive tags
def get_primary_iso_code(lang):
    try:
        result = (
            pycountry.languages.get(name=lang)
            or pycountry.languages.lookup(lang)
        )
        return result.alpha_2.lower()
    except Exception:
        return lang.lower()[:2]  # fallback to first 2 letters


# === SONARR ===
def get_sonarr_id(path):
    try:
        resp = requests.get(f"{SONARR_URL}/api/v3/series", headers={"X-Api-Key": SONARR_API_KEY})
        for s in resp.json():
            if os.path.basename(s['path']) == os.path.basename(path):
                return s['id']
    except Exception as e:
        logger.warning(f"Sonarr lookup failed: {e}")
    return None

def tag_sonarr(series_id, tag, remove=False, dry_run=False):
    if dry_run:
        logger.info(f"[Dry Run] Would {'remove' if remove else 'add'} tag '{tag}' for series ID {series_id}")
        return
    try:
        tag_id = None
        r = requests.get(f"{SONARR_URL}/api/v3/tag", headers={"X-Api-Key": SONARR_API_KEY})
        for t in r.json():
            if t["label"].lower() == tag.lower():
                tag_id = t["id"]
        if tag_id is None and not remove:
            r = requests.post(f"{SONARR_URL}/api/v3/tag", headers={"X-Api-Key": SONARR_API_KEY}, json={"label": tag})
            tag_id = r.json()["id"]
            logger.debug(f"Created new Sonarr tag '{tag}' with ID {tag_id}")

        s_url = f"{SONARR_URL}/api/v3/series/{series_id}"
        s_data = requests.get(s_url, headers={"X-Api-Key": SONARR_API_KEY}).json()
        if remove and tag_id in s_data["tags"]:
            s_data["tags"].remove(tag_id)
            logger.debug(f"Removing Sonarr tag ID {tag_id} from series {series_id}")
        elif not remove and tag_id not in s_data["tags"]:
            s_data["tags"].append(tag_id)
            logger.debug(f"Adding Sonarr tag ID {tag_id} to series {series_id}")
        requests.put(s_url, headers={"X-Api-Key": SONARR_API_KEY}, json=s_data)
        time.sleep(0.5)
    except Exception as e:
        logger.warning(f"Failed to tag Sonarr: {e}")

def refresh_sonarr_series(series_id, dry_run=False):
    if dry_run:
        logger.info(f"[Dry Run] Would trigger Sonarr refresh for series ID {series_id}")
        return
    try:
        url = f"{SONARR_URL}/api/v3/command"
        payload = {"name": "RefreshSeries", "seriesId": series_id}
        requests.post(url, json=payload, headers={"X-Api-Key": SONARR_API_KEY}, timeout=10)
        logger.debug(f"Sonarr refresh triggered for series ID: {series_id}")
    except Exception as e:
        logger.warning(f"Failed to trigger Sonarr refresh for {series_id}: {e}")

def get_sonarr_series(path):
    try:
        resp = requests.get(f"{SONARR_URL}/api/v3/series", headers={"X-Api-Key": SONARR_API_KEY})
        for s in resp.json():
            if os.path.basename(s['path']) == os.path.basename(path):
                return s
    except Exception as e:
        logger.warning(f"Failed to fetch Sonarr series metadata: {e}")
    return None


# === RADARR ===
def get_radarr_movies():
    """Fetch all movies from Radarr API."""
    try:
        resp = requests.get(f"{RADARR_URL}/api/v3/movie", headers={"X-Api-Key": RADARR_API_KEY})
        return resp.json()
    except Exception as e:
        logger.warning(f"Failed to fetch Radarr movies: {e}")
        return []


def get_radarr_movie_by_path(path):
    """Find a specific movie by its folder path."""
    try:
        resp = requests.get(f"{RADARR_URL}/api/v3/movie", headers={"X-Api-Key": RADARR_API_KEY})
        for m in resp.json():
            if os.path.basename(m['path']) == os.path.basename(path):
                return m
    except Exception as e:
        logger.warning(f"Radarr lookup failed: {e}")
    return None


def tag_radarr(movie_id, tag, remove=False, dry_run=False):
    """Add or remove a tag from a movie in Radarr."""
    if dry_run:
        logger.info(f"[Dry Run] Would {'remove' if remove else 'add'} tag '{tag}' for movie ID {movie_id}")
        return
    try:
        tag_id = None
        r = requests.get(f"{RADARR_URL}/api/v3/tag", headers={"X-Api-Key": RADARR_API_KEY})
        for t in r.json():
            if t["label"].lower() == tag.lower():
                tag_id = t["id"]
        if tag_id is None and not remove:
            r = requests.post(f"{RADARR_URL}/api/v3/tag", headers={"X-Api-Key": RADARR_API_KEY}, json={"label": tag})
            tag_id = r.json()["id"]
            logger.debug(f"Created new Radarr tag '{tag}' with ID {tag_id}")

        m_url = f"{RADARR_URL}/api/v3/movie/{movie_id}"
        m_data = requests.get(m_url, headers={"X-Api-Key": RADARR_API_KEY}).json()
        if remove and tag_id in m_data["tags"]:
            m_data["tags"].remove(tag_id)
            logger.debug(f"Removing Radarr tag ID {tag_id} from movie {movie_id}")
        elif not remove and tag_id not in m_data["tags"]:
            m_data["tags"].append(tag_id)
            logger.debug(f"Adding Radarr tag ID {tag_id} to movie {movie_id}")
        requests.put(m_url, headers={"X-Api-Key": RADARR_API_KEY}, json=m_data)
        time.sleep(0.5)
    except Exception as e:
        logger.warning(f"Failed to tag Radarr: {e}")


def safe_parse_nfo(path): #function to read carefully corrupted nfo files
    with open(path, "r", encoding="utf-8") as f:
        content = f.read()
    if "</tvshow>" in content:
        content = content.split("</tvshow>")[0] + "</tvshow>"
    return ET.fromstring(content)

# === Tagging ====

def update_nfo_tag(nfo_path, tag_value, dry_run=False):
    """
    Updates <tag> in the NFO file.
    Ensures the provided tag_value is the first <tag> in the list.
    Removes old tags from this system (dub, semi-dub, wrong-dub).
    """
    try:
        tree = ET.parse(nfo_path)
        root = tree.getroot()

        # Tags to manage
        known_tags = {"dub", "semi-dub", "wrong-dub"}

        # Remove any existing known tags
        old_tags = root.findall("tag")
        for t in old_tags:
            if t.text and t.text.strip().lower() in known_tags:
                root.remove(t)

        # Add new tag as first tag
        new_tag = ET.Element("tag")
        new_tag.text = tag_value
        insert_index = 0

        # Insert before existing <tag> if any
        for i, elem in enumerate(root):
            if elem.tag == "tag":
                insert_index = i
                break
        root.insert(insert_index, new_tag)

        if not dry_run:
            ET.indent(tree, space="  ")
            tree.write(nfo_path, encoding="utf-8", xml_declaration=False)
            logger.info(f"üè∑Ô∏è Updated <tag>{tag_value}</tag> in NFO: {os.path.basename(nfo_path)}")
        else:
            logger.info(f"[Dry Run] Would update <tag>{tag_value}</tag> in NFO: {os.path.basename(nfo_path)}")
    except Exception as e:
        logger.warning(f"‚ùå Failed to update <tag> in NFO: {e}")


# === MAIN FUNCTION ===
def run_loop(opts):
    while True:
        main(opts)
        time.sleep(RUN_INTERVAL_SECONDS)

def main(opts=None):
    logger.info(f"üè∑Ô∏è Taggarr - {__description__}")
    time.sleep(1)
    logger.info(f"üè∑Ô∏è Taggarr - v{__version__} started.")
    time.sleep(1)
    logger.info("Starting Taggarr scan...")
    time.sleep(5)
    if opts is None:
        parser = argparse.ArgumentParser()
        parser.add_argument('--write-mode', type=int, choices=[0, 1, 2], default=int(os.getenv("WRITE_MODE", 0)), help="0 = default, 1 = rewrite all, 2 = remove all")
        parser.add_argument('--quick', action='store_true')
        parser.add_argument('--dry-run', action='store_true')
        opts = parser.parse_args()
    env_vars = {key: os.getenv(key) for key in ["WRITE_MODE", "QUICK_MODE", "DRY_RUN", "TARGET_GENRE", "ROOT_TV_PATH", "TARGET_LANGUAGES", "START_RUNNING"]}
    logger.debug(f"Environment variables: {env_vars}...")
    #logger.debug(f"Initializing with options: {opts}...")
    time.sleep(3)
    quick_mode = opts.quick or QUICK_MODE
    dry_run = opts.dry_run or DRY_RUN
    write_mode = opts.write_mode or WRITE_MODE

    if quick_mode:
        logger.info("Quick mode is enabled: Scanning only the first episode of each season.")
    if dry_run:
        logger.info("Dry run mode is enabled: No Sonarr API calls or .nfo file edits will be made.")
    if write_mode == 0:
        logger.info("Write mode is set to 0 or none. Processing shows as usual.")
    if write_mode == 1:
        logger.info("Rewrite mode is enabled: Everything will be rebuilt.")
    if write_mode == 2:
        logger.info("Remove mode is enabled: Everything will be removed.")

    current_mtime = 0
    taggarr = load_taggarr()
    logger.debug(f"Available paths in JSON: {list(taggarr['series'].keys())[:5]}")

    for show in sorted(os.listdir(ROOT_TV_PATH)):
        show_path = os.path.join(ROOT_TV_PATH, show)
        show_path = os.path.abspath(show_path)
        if not os.path.isdir(show_path):
            continue
        normalized_path = show_path
        show_meta = taggarr["series"].get(os.path.abspath(show_path), {})
        saved_seasons = taggarr["series"].get(normalized_path, {}).get("seasons", {})
        changed = False

        for d in os.listdir(show_path):
            season_path = os.path.join(show_path, d)
            if os.path.isdir(season_path) and d.lower().startswith("season"):
                current_mtime = os.path.getmtime(season_path)
                saved_mtime = saved_seasons.get(d, {}).get("last_modified", 0)
                if current_mtime > saved_mtime:
                    changed = True
                    break

        # NEW: detect new show
        is_new_show = normalized_path not in taggarr["series"]

        # NEW: detect new season folder
        existing_seasons = set(saved_seasons.keys())
        current_seasons = set(d for d in os.listdir(show_path) if os.path.isdir(os.path.join(show_path, d)) and d.lower().startswith("season"))
        new_season_detected = len(current_seasons - existing_seasons) > 0

        if write_mode == 0 and not (changed or is_new_show or new_season_detected):
            logger.info(f"üö´ Skipping {show} - no new or updated seasons")
            continue


        nfo_path = os.path.join(show_path, "tvshow.nfo")
        if not os.path.exists(nfo_path):
            logger.debug(f"No NFO found for: {show}")
            continue

        try:
            root = safe_parse_nfo(nfo_path)
            genres = [g.text.lower() for g in root.findall("genre")]
            if TARGET_GENRE and TARGET_GENRE.lower() not in genres:
                logger.info(f"üö´‚õî Skipping {show}: genre mismatch")
                continue
        except Exception as e:
            logger.warning(f"Genre parsing failed for {show}: {e}")
            continue

        logger.info(f"üì∫ Processing show: {show}")

        sid = get_sonarr_id(show_path)
        if not sid:
            logger.warning(f"No Sonarr ID for {show}")
            continue

        if write_mode == 2:
            logger.info(f"Removing tags for {show}")
            for tag in [TAG_DUB, TAG_SEMI, TAG_WRONG_DUB]:
                tag_sonarr(sid, tag, remove=True, dry_run=dry_run)
            if show_path in taggarr["series"]:
                del taggarr["series"][show_path]
            continue

        series_data = get_sonarr_series(show_path)
        if not series_data:
            logger.warning(f"No Sonarr metadata found for {show}")
            continue

        tag, seasons = determine_tag_and_stats(show_path, series_data, quick=quick_mode)

        original_lang_raw = series_data.get("originalLanguage", "")
        if isinstance(original_lang_raw, dict):
            original_lang = original_lang_raw.get("name", "").lower()
        else:
            original_lang = str(original_lang_raw).lower()

        logger.info(f"üè∑Ô∏è‚úÖ Tagged as {tag if tag else 'no tag (original)'}")

        if tag: #tag handling
            tag_sonarr(sid, tag, dry_run=dry_run)
            if tag == TAG_WRONG_DUB:
                tag_sonarr(sid, TAG_SEMI, remove=True, dry_run=dry_run)
                tag_sonarr(sid, TAG_DUB, remove=True, dry_run=dry_run)
            elif tag == TAG_SEMI:
                tag_sonarr(sid, TAG_WRONG_DUB, remove=True, dry_run=dry_run)
                tag_sonarr(sid, TAG_DUB, remove=True, dry_run=dry_run)
            elif tag == TAG_DUB:
                tag_sonarr(sid, TAG_WRONG_DUB, remove=True, dry_run=dry_run)
                tag_sonarr(sid, TAG_SEMI, remove=True, dry_run=dry_run)
            else:
                logger.info(f"Removing all tags from {show} since it's original (no tag)")
                for t in [TAG_DUB, TAG_SEMI, TAG_WRONG_DUB]:
                    tag_sonarr(sid, t, remove=True, dry_run=dry_run)

        # Add <genre>Dub</genre> to .nfo if ADD_TAG_TO_GENRE is true and it's fully dubbed
        if ADD_TAG_TO_GENRE:
            try:
                tree = ET.parse(nfo_path)
                root = tree.getroot()
                genres = [g.text.strip().lower() for g in root.findall("genre") if g.text]

                # üí° Early exit if no changes needed
                if (tag == TAG_DUB and "dub" in genres) or (tag != TAG_DUB and "dub" not in genres):
                    logger.debug(f"‚ÑπÔ∏è No NFO genre update needed for {show}")
                else:
                    modified = False

                    if tag == TAG_DUB and "dub" not in genres:
                        new_genre = ET.Element("genre")
                        new_genre.text = "Dub"
                        first_genre = root.find("genre")
                        if first_genre is not None:
                            root.insert(list(root).index(first_genre), new_genre)
                        else:
                            root.append(new_genre)
                        modified = True
                        logger.info(f"üìÑ Will add <genre>Dub</genre> to NFO for {show}")

                    elif tag != TAG_DUB and "dub" in genres:
                        for g in root.findall("genre"):
                            if g.text and g.text.strip().lower() == "dub":
                                root.remove(g)
                                modified = True
                        logger.info(f"üìÑ Will remove <genre>Dub</genre> from NFO for {show}")

                    if modified and not dry_run:
                        ET.indent(tree, space="  ")
                        tree.write(nfo_path, encoding="utf-8", xml_declaration=False)
                    elif modified and dry_run:
                        logger.info(f"[Dry Run] Would update NFO file for {show}")

            except Exception as e:
                logger.warning(f"‚ùå Failed to update NFO genre for {show}: {e}")
        # Unsures <tag>dub</tag> to .nfo
        if tag in [TAG_DUB, TAG_SEMI, TAG_WRONG_DUB]:
            update_nfo_tag(nfo_path, tag, dry_run=dry_run)


        taggarr["series"][normalized_path] = {
            "display_name": show,
            "tag": tag or "none",
            "last_scan": datetime.utcnow().isoformat() + "Z",
            "original_language": original_lang,
            "seasons": seasons,
            "last_modified": current_mtime
        }
        logger.debug(f"Normalized show_path: {show_path}")
        logger.debug(f"Saved series info under normalized path: {normalized_path}")
        if write_mode == 1:
            refresh_sonarr_series(sid, dry_run=dry_run)
            time.sleep(0.5)

    save_taggarr(taggarr)
    logger.info("‚úÖ Finished Taggarr scan.")
    logger.info(f"‚ÑπÔ∏è You don't have all the dubs? Checkout Huntarr.io to hunt them for you!")
    logger.info(f"Next scan is in {RUN_INTERVAL_SECONDS/60/60} hours.")


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--write-mode', type=int, choices=[0, 1, 2], default=int(os.getenv("WRITE_MODE", 0)), help="0 = default, 1 = rewrite all, 2 = remove all")
    parser.add_argument('--quick', action='store_true')
    parser.add_argument('--dry-run', action='store_true')
    opts = parser.parse_args()

    if START_RUNNING:
        run_loop(opts)
    elif any(vars(opts).values()):
        logger.debug("CLI args passed. Running one-time scan...")
        main(opts)
    else:
        logger.debug("START_RUNNING is false and no CLI args passed. Waiting for commands...")
        while True:
            time.sleep(RUN_INTERVAL_SECONDS)
